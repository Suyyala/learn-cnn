{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "AaqKOSCjsGpT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple convolution operation\n",
        "\n",
        "A = torch.tensor([[1, 2], [3, 4]])\n",
        "B = torch.tensor([[1, 0], [0, 1]])\n",
        "\n",
        "C = A * B\n",
        "print(C)\n",
        "print(torch.sum(C))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_SbnFinwtfy",
        "outputId": "f6932384-9016-4ce9-865a-8118768c5a67"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 0],\n",
            "        [0, 4]])\n",
            "tensor(5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolution with stride\n",
        "def conv2d(A, K):\n",
        "  a_h, a_w = A.shape\n",
        "  k_h, k_w = K.shape\n",
        "  o_h = a_h - k_h + 1\n",
        "  o_w = a_w - k_w +1\n",
        "\n",
        "  out = torch.zeros((o_h, o_w))\n",
        "\n",
        "  for i in range(o_h):\n",
        "    for j in range(o_w):\n",
        "      c = A[i:i+k_h, j:j + k_w] * K\n",
        "      out[i, j] = torch.sum(c)\n",
        "  return out\n",
        "\n",
        "\n",
        "a = torch.ones((4, 4))\n",
        "k = torch.ones((2,2))\n",
        "print(conv2d(a, k))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZXFzSmbyvxj",
        "outputId": "e54ac911-2186-4c23-ab57-27342abe7e15"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4., 4., 4.],\n",
            "        [4., 4., 4.],\n",
            "        [4., 4., 4.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# convolution with stride and padding\n",
        "# why do i need padding - to preserve information at the border\n",
        "# 2) to keep spatial dimention\n",
        "def conv2d(A, K, stride=1, padding=1):\n",
        "    # Pad the input tensor\n",
        "    A = F.pad(A, (padding, padding, padding, padding))\n",
        "\n",
        "    a_h, a_w = A.shape\n",
        "    k_h, k_w = K.shape\n",
        "\n",
        "    o_h = (a_h - k_h) // stride + 1\n",
        "    o_w = (a_w - k_w) // stride + 1\n",
        "\n",
        "    out = torch.zeros((o_h, o_w))\n",
        "\n",
        "    for i in range(0, o_h, stride):\n",
        "        for j in range(0, o_w, stride):\n",
        "            c = A[i: i + k_h, j: j + k_w] * K\n",
        "            out[i, j] = torch.sum(c)\n",
        "\n",
        "    return out\n",
        "\n",
        "a = torch.ones((5, 5))\n",
        "k = torch.ones((3, 3))\n",
        "\n",
        "print(conv2d(a, k, stride=2, padding=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koZcu4A0rQkD",
        "outputId": "53a3dadb-00be-4b17-f773-1a6f28c27901"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 3., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [3., 0., 9., 0.],\n",
            "        [0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 2D convolution layer\n",
        "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride=1, padding=2)\n",
        "\n",
        "# Manually set the weights and bias of the convolution layer\n",
        "conv.weight.data = torch.Tensor([[[[1, 2], [3, 4]]]])\n",
        "conv.bias.data.fill_(0)\n",
        "\n",
        "# Create a 3x3 input with a batch size of 1\n",
        "input = torch.Tensor([[[[1, 1, 1], [1, 1, 1], [1, 1, 1]]]])\n",
        "output = conv(input)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udshhgHToWD4",
        "outputId": "a6305056-2eb0-4143-b15d-07df255d6d1c"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
            "          [ 0.,  4.,  7.,  7.,  3.,  0.],\n",
            "          [ 0.,  6., 10., 10.,  4.,  0.],\n",
            "          [ 0.,  6., 10., 10.,  4.,  0.],\n",
            "          [ 0.,  2.,  3.,  3.,  1.,  0.],\n",
            "          [ 0.,  0.,  0.,  0.,  0.,  0.]]]], grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step1: Define the architecture\n",
        "# conv = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "\n",
        "\n",
        "class Conv2d(nn.Module):\n",
        "  def __init__(self, in_channles, out_channels=1, kernel_size=3, stride=1, padding=0 ):\n",
        "    self.k = torch.randn((kernel_size, kernel_size))\n",
        "\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.forward(x)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # create feature map for with each filter by applying convolution\n",
        "    return x\n",
        ""
      ],
      "metadata": {
        "id": "FiQhMTyYzpYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LearnCNN(nn.Module):\n",
        "  def __init__(self, batch_size, filters=1, kernel_size=3, stride=1, padding=0)"
      ],
      "metadata": {
        "id": "CkOUr37aa37i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils import data\n",
        "# step2: Data preparation - MNIST\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Download dataset\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "full_data = datasets.MNIST('./data', download=True, transform=transform)\n",
        "\n",
        "# Proprocess dataset\n",
        "train_size = int(len(full_data) * 0.7)\n",
        "test_size = int(len(full_data) * 0.2)\n",
        "val_size = int(len(full_data) * 0.1)\n",
        "\n",
        "# Split dataset\n",
        "train_set, test_set, val_set = random_split(full_data, [train_size, test_size, val_size])\n",
        "print(len(train_set), len(test_set), len(val_set))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN1BBfzMKObb",
        "outputId": "30e7620b-a692-4fce-ff4d-3e5574beec6f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42000 12000 6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders for each dataset\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "lAx7UEexVt9e"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_loader:\n",
        "    print(images.shape, labels.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVuGe5UVW7wX",
        "outputId": "12a161c3-4280-42c3-9724-9f7408b60836"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get a batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Move the images to cpu\n",
        "generated_images = images[:4]\n",
        "\n",
        "# Create a grid of images and convert it to numpy\n",
        "grid = torchvision.utils.make_grid(generated_images, nrow=8)\n",
        "npimg = grid.numpy()\n",
        "\n",
        "# Plot the grid of images\n",
        "# Plot the images\n",
        "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "-r_bOL6lUYHO",
        "outputId": "2032023a-58d7-461b-8b25-3217c84242ac"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step3: Training\n",
        "\n",
        "# hyper parameters\n",
        "\n",
        "batch_size = 32\n",
        "channels = 3\n",
        "width = 64\n",
        "height = 64\n",
        "\n",
        "\n",
        "conv = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "# training loop\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.adam(conv.parameters())\n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "  for batch in train_loader:\n",
        "    y_preds = nn.conv2d(batch)\n",
        "    # why ? loss -negative log likely hood\n",
        "    # logits\n",
        "    y_preds = F.softmax(y_preds)\n",
        "    # loss calculation\n",
        "    loss = loss_func(y_preds, y)\n",
        "    # zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "    # calculate graidents\n",
        "    loss.backward()\n",
        "    # update weights\n",
        "    optimizer.step()\n",
        "    # Print the loss for this epoch\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "zb1YI8o31AIg",
        "outputId": "0d15f829-440a-4c0b-d54c-499c8dedebd5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-12e6f6c5cc11>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch.optim' has no attribute 'adam'"
          ]
        }
      ]
    }
  ]
}